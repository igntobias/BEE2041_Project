{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6392af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7684f",
   "metadata": {},
   "source": [
    "### Obtaining all Names (Wikia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ad02c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlnames = \"https://game8.co/games/Genshin-Impact/archives/296707\"\n",
    "\n",
    "#Web scraping the wikia for Genshin's playable character list.\n",
    "\n",
    "urlnames_wk = \"https://genshin-impact.fandom.com/wiki/Category:Playable_Characters\"\n",
    "\n",
    "pagenames_wk = requests.get(urlnames_wk)\n",
    "\n",
    "soupname_wk = BeautifulSoup(pagenames_wk.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a05a2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping through to get the html, identifying which class and identifier to find the names\n",
    "\n",
    "imgtag_wk = soupname_wk.find_all(\"a\", class_=\"category-page__member-link\")\n",
    "#imgtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f326d3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albedo', 'Alhaitham', 'Aloy', 'Amber', 'Arataki_Itto', 'Baizhu', 'Barbara', 'Beidou', 'Bennett', 'Candace', 'Charlotte', 'Chevreuse', 'Chiori', 'Chongyun', 'Collei', 'Cyno', 'Dehya', 'Diluc', 'Diona', 'Dori', 'Eula', 'Faruzan', 'Fischl', 'Freminet', 'Furina', 'Gaming', 'Ganyu', 'Gorou', 'Hu_Tao', 'Jean', 'Kaedehara_Kazuha', 'Kaeya', 'Kamisato_Ayaka', 'Kamisato_Ayato', 'Kaveh', 'Keqing', 'Kirara', 'Klee', 'Kujou_Sara', 'Kuki_Shinobu', 'Layla', 'Lisa', 'Lynette', 'Lyney', 'Mika', 'Mona', 'Nahida', 'Navia', 'Neuvillette', 'Nilou', 'Ningguang', 'Noelle', 'Qiqi', 'Raiden_Shogun', 'Razor', 'Rosaria', 'Sangonomiya_Kokomi', 'Sayu', 'Shenhe', 'Shikanoin_Heizou', 'Sucrose', 'Tartaglia', 'Thoma', 'Tighnari', 'Traveler_(Anemo)', 'Traveler_(Dendro)', 'Traveler_(Electro)', 'Traveler_(Geo)', 'Traveler_(Hydro)', 'Venti', 'Wanderer', 'Wriothesley', 'Xiangling', 'Xianyun', 'Xiao', 'Xingqiu', 'Xinyan', 'Yae_Miko', 'Yanfei', 'Yaoyao', 'Yelan', 'Yoimiya', 'Yun_Jin', 'Zhongli']\n"
     ]
    }
   ],
   "source": [
    "#Converting the soup into a usable List\n",
    "\n",
    "type(imgtag_wk)\n",
    "\n",
    "titles = [tag['title'] for tag in imgtag_wk]\n",
    "characters_wk = titles[6:]\n",
    "\n",
    "#Replacing spaces with underscores for it to be usable in iterating through the wikia pages\n",
    "\n",
    "converter = lambda x: x.replace(' ', '_')\n",
    "characters_wk = list(map(converter, characters_wk))\n",
    "characters_wk = sorted(characters_wk)\n",
    "\n",
    "\n",
    "remove = [\"Lumine\", \"Traveler_(Unaligned)\", \"Traveler\"]    #These characters are extras\n",
    "\n",
    "characters_wk = [item for item in characters_wk if item not in remove]\n",
    "\n",
    "print(characters_wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd05956",
   "metadata": {},
   "source": [
    "### Obtaining All Names (gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc5fffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web scraping the wikia for Genshin's playable character list.\n",
    "\n",
    "urlnames_gg = \"https://genshin.gg/\"\n",
    "\n",
    "pagenames_gg = requests.get(urlnames_gg)\n",
    "\n",
    "soupname_gg = BeautifulSoup(pagenames_gg.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff9e584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping through to get the html, identifying which class and identifier to find the names\n",
    "\n",
    "imgtag_gg = soupname_gg.find_all(\"h2\", class_= \"character-name\")\n",
    "#imgtag_gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "509af59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albedo', 'alhaitham', 'aloy', 'amber', 'ayaka', 'ayato', 'baizhu', 'barbara', 'beidou', 'bennett', 'candace', 'charlotte', 'chevreuse', 'childe', 'chiori', 'chongyun', 'collei', 'cyno', 'dehya', 'diluc', 'diona', 'dori', 'eula', 'faruzan', 'fischl', 'freminet', 'furina', 'gaming', 'ganyu', 'gorou', 'heizou', 'hutao', 'itto', 'jean', 'kaeya', 'kaveh', 'kazuha', 'keqing', 'kirara', 'klee', 'kokomi', 'kukishinobu', 'layla', 'lisa', 'lynette', 'lyney', 'mika', 'mona', 'nahida', 'navia', 'neuvillette', 'nilou', 'ningguang', 'noelle', 'qiqi', 'raiden', 'razor', 'rosaria', 'sara', 'sayu', 'shenhe', 'sucrose', 'thoma', 'tighnari', 'traveler(anemo)', 'traveler(dendro)', 'traveler(electro)', 'traveler(geo)', 'traveler(hydro)', 'venti', 'wanderer', 'wriothesley', 'xiangling', 'xianyun', 'xiao', 'xingqiu', 'xinyan', 'yaemiko', 'yanfei', 'yaoyao', 'yelan', 'yoimiya', 'yunjin', 'zhongli']\n"
     ]
    }
   ],
   "source": [
    "characters_gg = [element.get_text() for element in imgtag_gg]\n",
    "#characters\n",
    "\n",
    "def process_names(names_list):\n",
    "    return [name.replace(\" \", \"\").lower() for name in names_list]\n",
    "\n",
    "characters_gg = process_names(characters_gg)\n",
    "characters_gg = sorted(characters_gg)\n",
    "print(characters_gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9afdc7d",
   "metadata": {},
   "source": [
    "### Tidying Up Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a996843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(characters_wk)\n",
    "#len(characters_gg)\n",
    "\n",
    "#[characters_gg, characters_wk]\n",
    "#names = arr(characters_gg & characters_wk)\n",
    "#names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88c68e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(characters_gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67a6b2ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(characters_wk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfaeb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albedo', 'Alhaitham', 'Aloy', 'Amber', 'Baizhu', 'Barbara', 'Beidou', 'Bennett', 'Candace', 'Charlotte', 'Chevreuse', 'Chiori', 'Chongyun', 'Collei', 'Cyno', 'Dehya', 'Diluc', 'Diona', 'Dori', 'Eula', 'Faruzan', 'Fischl', 'Freminet', 'Furina', 'Gaming', 'Ganyu', 'Gorou', 'Hu_Tao', 'Jean', 'Kaeya', 'Kaveh', 'Keqing', 'Kirara', 'Klee', 'Kuki_Shinobu', 'Layla', 'Lisa', 'Lynette', 'Lyney', 'Mika', 'Mona', 'Nahida', 'Navia', 'Neuvillette', 'Nilou', 'Ningguang', 'Noelle', 'Qiqi', 'Raiden_Shogun', 'Razor', 'Rosaria', 'Sayu', 'Shenhe', 'Sucrose', 'Thoma', 'Tighnari', 'Traveler_(Anemo)', 'Traveler_(Dendro)', 'Traveler_(Electro)', 'Traveler_(Geo)', 'Traveler_(Hydro)', 'Venti', 'Wanderer', 'Wriothesley', 'Xiangling', 'Xianyun', 'Xiao', 'Xingqiu', 'Xinyan', 'Yae_Miko', 'Yanfei', 'Yaoyao', 'Yelan', 'Yoimiya', 'Yun_Jin', 'Zhongli', 'Arataki_Itto', 'Kaedehara_Kazuha', 'Shikanoin_Heizou', 'Kujou_Sara', 'Tartaglia', 'Kamisato_Ayato', 'Kamisato_Ayaka', 'Sangonomiya_Kokomi']\n",
      "['albedo', 'alhaitham', 'aloy', 'amber', 'baizhu', 'barbara', 'beidou', 'bennett', 'candace', 'charlotte', 'chevreuse', 'chiori', 'chongyun', 'collei', 'cyno', 'dehya', 'diluc', 'diona', 'dori', 'eula', 'faruzan', 'fischl', 'freminet', 'furina', 'gaming', 'ganyu', 'gorou', 'hutao', 'jean', 'kaeya', 'kaveh', 'keqing', 'kirara', 'klee', 'kukishinobu', 'layla', 'lisa', 'lynette', 'lyney', 'mika', 'mona', 'nahida', 'navia', 'neuvillette', 'nilou', 'ningguang', 'noelle', 'qiqi', 'raiden', 'razor', 'rosaria', 'sayu', 'shenhe', 'sucrose', 'thoma', 'tighnari', 'traveler(anemo)', 'traveler(dendro)', 'traveler(electro)', 'traveler(geo)', 'traveler(hydro)', 'venti', 'wanderer', 'wriothesley', 'xiangling', 'xianyun', 'xiao', 'xingqiu', 'xinyan', 'yaemiko', 'yanfei', 'yaoyao', 'yelan', 'yoimiya', 'yunjin', 'zhongli', 'itto', 'kazuha', 'heizou', 'sara', 'childe', 'ayato', 'ayaka', 'kokomi']\n"
     ]
    }
   ],
   "source": [
    "remove_wk = [\"Arataki_Itto\", \"Kaedehara_Kazuha\", \"Shikanoin_Heizou\", \"Kujou_Sara\", \"Tartaglia\",\n",
    "         \"Kamisato_Ayato\", \"Kamisato_Ayaka\", \"Sangonomiya_Kokomi\"]\n",
    "remove_gg = [\"itto\", \"kazuha\", \"heizou\", \"sara\", \"childe\", \"ayato\", \"ayaka\", \"kokomi\"]\n",
    "\n",
    "characters_wk = [item for item in characters_wk if item not in remove_wk]\n",
    "characters_wk = characters_wk + remove_wk\n",
    "print(characters_wk)\n",
    "\n",
    "characters_gg = [item for item in characters_gg if item not in remove_gg]\n",
    "characters_gg = characters_gg + remove_gg\n",
    "print(characters_gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "339eddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(50,80):\n",
    "#    print(characters_gg2[i])\n",
    "#    print(characters_wk2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8c0ce",
   "metadata": {},
   "source": [
    "### Scraping Constellation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c1266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Scraping Constellation Data from Genshin.gg - Complete\n",
    "\n",
    "characters_gg_test = characters_gg[2:5]\n",
    "cons_full = []\n",
    "\n",
    "for names in characters_gg_test:\n",
    "    \n",
    "    url_chara_gg = \"https://genshin.gg/characters/\"+names+\"/\"\n",
    "    page_chara_gg = requests.get(url_chara_gg)\n",
    "    soup_chara_gg = BeautifulSoup(page_chara_gg.content, \"html.parser\")\n",
    " \n",
    "\n",
    "    #Obtaining the constellation html info\n",
    "    \n",
    "    if soup_chara_gg.find(\"div\", id='constellations') != None:\n",
    "        cons1 = soup_chara_gg.find(\"div\", id=\"constellations\").find_all(\"div\", class_=\"character-skill-description\")\n",
    "        cons2 = [con.get_text() for con in cons1]\n",
    "        #print(cons2)\n",
    "        cons_full.append(cons2)\n",
    "    \n",
    "    else:\n",
    "        #print(\"No Constellations\")\n",
    "        notext = \"This character has no constellations.\"\n",
    "        cons_full.append(notext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa2760ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cons_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f37d680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m release_date_div \u001b[38;5;241m=\u001b[39m soup_chara_wk\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-source\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreleaseDate\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(release_date_div))\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrelease_date_div\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpi-data-value\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     17\u001b[0m output[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m2\u001b[39m][:\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "#Obtaining the release date of the Character\n",
    "\n",
    "characters_wk_test = characters_wk[74]\n",
    "dates_full = []\n",
    "\n",
    "for names in characters_wk_test:\n",
    "\n",
    "    url_chara_wk = \"https://genshin-impact.fandom.com/wiki/\"+names\n",
    "    page_chara_wk = requests.get(url_chara_wk)\n",
    "    soup_chara_wk = BeautifulSoup(page_chara_wk.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    release_date_div = soup_chara_wk.find('div', {'data-source': 'releaseDate'})\n",
    "    print(type(release_date_div))\n",
    "    \n",
    "    output = release_date_div.find('div', class_='pi-data-value').get_text(strip=True).split()[0:3]\n",
    "    output[2] = output[2][:4]\n",
    "    \n",
    "    output = ' '.join(output)\n",
    "    output = datetime.strptime(output, \"%B %d, %Y\").date()\n",
    "    \n",
    "    dates_full.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "883ddfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_date_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d93cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #if release_date_div:\n",
    "    #    release_date_text = release_date_div.find('div', class_='pi-data-value')\n",
    "    #    release_date_text = release_date_text.get_text(strip=True).split(' ')\n",
    "    #    output = release_date_text[0:3]\n",
    "    \n",
    "    #    if len(output) >= 3:                  # Check if there are at least 3 elements in the list\n",
    "    #        output[2] = output[2][:4]\n",
    "\n",
    "        #output = ' '.join(output)\n",
    "        #output = datetime.strptime(output, \"%B %d, %Y\").date()\n",
    "    #    dates_full.append(output)\n",
    "        \n",
    "    #else:\n",
    "    #    print(\"No release date found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d597c07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sangonomiya_Kokomi'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_wk[83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d7166bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_chara_wk = \"https://genshin-impact.fandom.com/wiki/\"+characters_wk[83]\n",
    "page_chara_wk = requests.get(url_chara_wk)\n",
    "soup_chara_wk = BeautifulSoup(page_chara_wk.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd3b862e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"pi-item pi-data pi-item-spacing pi-border-color\" data-source=\"releaseDate\">\n",
       "<h3 class=\"pi-data-label pi-secondary-font\"><a href=\"/wiki/Character/List#Characters_by_Release_Date\" title=\"Character/List\">Release Date</a></h3>\n",
       "<div class=\"pi-data-value pi-font\">September 21, 2021<br/>2 years, 6 months ago</div>\n",
       "</div>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release_date_div = soup_chara_wk.find('div', {'data-source': 'releaseDate'})\n",
    "release_date_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f539b2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2021, 9, 21)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = release_date_div.find('div', class_='pi-data-value').get_text(strip=True).split()[0:3]\n",
    "output[2] = output[2][:4]\n",
    "\n",
    "output = ' '.join(output)\n",
    "output = datetime.strptime(output, \"%B %d, %Y\").date()\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd8759",
   "metadata": {},
   "source": [
    "### Obtaining list of all classes used in Character's html page\n",
    "\n",
    "#class list set \n",
    "class_list = set() \n",
    "\n",
    "#Page content from Website URL \n",
    "page = requests.get(url_chara) \n",
    "\n",
    "#parse html content \n",
    "soup = BeautifulSoup(page.content , 'html.parser') \n",
    "\n",
    "#get all tags \n",
    "tags = {tag.name for tag in soup.find_all()} \n",
    "\n",
    "\n",
    "for tag in tags:                       # iterate all tags \n",
    "\n",
    "    for i in soup.find_all( tag ):     # find all element of tag \n",
    " \n",
    "        if i.has_attr( \"class\" ):      # if tag has attribute of class\n",
    "\n",
    "            if len( i['class'] ) != 0: \n",
    "                class_list.add(\" \".join( i['class'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da5d22",
   "metadata": {},
   "source": [
    "#print(class_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of the character's constellations.\n",
    "\n",
    "cons3 = []\n",
    "\n",
    "for data in cons2:\n",
    "    \n",
    "    #soup = BeautifulSoup(data, 'html.parser')\n",
    "    #print(type(data))\n",
    "    #li_tag = data.find('li')\n",
    "    \n",
    "    if data:\n",
    "        data = data.text.strip()\n",
    "        #print(data)\n",
    "        cons3.append(data)\n",
    "    else:\n",
    "        print(\"No tag found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cons3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16ae2235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release Date: May 02, 2023\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the release date of the Character\n",
    "\n",
    "#print(release_date_div)\n",
    "\n",
    "\n",
    "url_chara_wk = \"https://genshin-impact.fandom.com/wiki/\"+characters_wk[4]\n",
    "page_chara_wk = requests.get(url_chara_wk)\n",
    "soup_chara_wk = BeautifulSoup(page_chara_wk.content, \"html.parser\")\n",
    "\n",
    "\n",
    "release_date_div = soup_chara_wk.find('div', {'data-source': 'releaseDate'})\n",
    "\n",
    "\n",
    "if release_date_div:\n",
    "    release_date_text = release_date_div.find('div', class_='pi-data-value')\n",
    "    release_date_text = release_date_text.get_text(strip=True).split(' ')\n",
    "    \n",
    "    #print(release_date_text)\n",
    "    #str(release_date_text)\n",
    "        \n",
    "    output = release_date_text[0:3]\n",
    "    \n",
    "    if len(output) >= 3:  # Check if there are at least 3 elements in the list\n",
    "        output[2] = output[2][:4]\n",
    "\n",
    "    output = ' '.join(output)\n",
    "    #output = datetime.strptime(output, \"%B %d, %Y\").date()\n",
    "    \n",
    "    #output\n",
    "    \n",
    "    print(\"Release Date:\", output)\n",
    "    \n",
    "else:\n",
    "    print(\"No release date found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a968d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = release_date_text[0:3]\n",
    "output[2][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481aa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string_to_remove = ['itto', 'kazuha', 'sara', 'kokomi', 'ayato', 'ayaka', 'childe']\n",
    "#filtered_list1 = [item for item in characters_gg if item != string_to_remove]\n",
    "\n",
    "#'Arataki_Itto', 'Kaedehara_Kazuha', \"Sangonomiya_Kokomi\", \"Kujo_Sara\", 'Kamisato_Ayato', 'Kamisato_Ayaka', \n",
    "#'Lumine', 'Traveler_(Unaligned)', Tartaglia\n",
    "\n",
    "#filtered_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcbab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
