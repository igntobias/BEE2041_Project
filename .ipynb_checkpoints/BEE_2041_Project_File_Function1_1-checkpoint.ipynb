{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6392af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7684f",
   "metadata": {},
   "source": [
    "### Obtaining all Names (Wikia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad02c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlnames = \"https://game8.co/games/Genshin-Impact/archives/296707\"\n",
    "\n",
    "#Web scraping the wikia for Genshin's playable character list.\n",
    "\n",
    "urlnames_wk = \"https://genshin-impact.fandom.com/wiki/Category:Playable_Characters\"\n",
    "\n",
    "pagenames_wk = requests.get(urlnames_wk)\n",
    "\n",
    "soupname_wk = BeautifulSoup(pagenames_wk.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05a2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping through to get the html, identifying which class and identifier to find the names\n",
    "\n",
    "imgtag_wk = soupname_wk.find_all(\"a\", class_=\"category-page__member-link\")\n",
    "#imgtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f326d3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albedo', 'Alhaitham', 'Aloy', 'Amber', 'Arataki_Itto', 'Baizhu', 'Barbara', 'Beidou', 'Bennett', 'Candace', 'Charlotte', 'Chevreuse', 'Chiori', 'Chongyun', 'Collei', 'Cyno', 'Dehya', 'Diluc', 'Diona', 'Dori', 'Eula', 'Faruzan', 'Fischl', 'Freminet', 'Furina', 'Gaming', 'Ganyu', 'Gorou', 'Hu_Tao', 'Jean', 'Kaedehara_Kazuha', 'Kaeya', 'Kamisato_Ayaka', 'Kamisato_Ayato', 'Kaveh', 'Keqing', 'Kirara', 'Klee', 'Kujou_Sara', 'Kuki_Shinobu', 'Layla', 'Lisa', 'Lumine', 'Lynette', 'Lyney', 'Mika', 'Mona', 'Nahida', 'Navia', 'Neuvillette', 'Nilou', 'Ningguang', 'Noelle', 'Qiqi', 'Raiden_Shogun', 'Razor', 'Rosaria', 'Sangonomiya_Kokomi', 'Sayu', 'Shenhe', 'Shikanoin_Heizou', 'Sucrose', 'Tartaglia', 'Thoma', 'Tighnari', 'Traveler', 'Traveler_(Anemo)', 'Traveler_(Dendro)', 'Traveler_(Electro)', 'Traveler_(Geo)', 'Traveler_(Hydro)', 'Traveler_(Unaligned)', 'Venti', 'Wanderer', 'Wriothesley', 'Xiangling', 'Xianyun', 'Xiao', 'Xingqiu', 'Xinyan', 'Yae_Miko', 'Yanfei', 'Yaoyao', 'Yelan', 'Yoimiya', 'Yun_Jin', 'Zhongli']\n"
     ]
    }
   ],
   "source": [
    "#Converting the soup into a usable List\n",
    "\n",
    "type(imgtag_wk)\n",
    "\n",
    "titles = [tag['title'] for tag in imgtag_wk]\n",
    "characters_wk = titles[6:]\n",
    "\n",
    "#Replacing spaces with underscores for it to be usable in iterating through the wikia pages\n",
    "\n",
    "converter = lambda x: x.replace(' ', '_')\n",
    "characters_wk = list(map(converter, characters_wk))\n",
    "characters_wk = sorted(characters_wk)\n",
    "print(characters_wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd05956",
   "metadata": {},
   "source": [
    "### Obtaining All Names (gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc5fffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web scraping the wikia for Genshin's playable character list.\n",
    "\n",
    "urlnames_gg = \"https://genshin.gg/\"\n",
    "\n",
    "pagenames_gg = requests.get(urlnames_gg)\n",
    "\n",
    "soupname_gg = BeautifulSoup(pagenames_gg.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9e584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping through to get the html, identifying which class and identifier to find the names\n",
    "\n",
    "imgtag_gg = soupname_gg.find_all(\"h2\", class_= \"character-name\")\n",
    "#imgtag_gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "509af59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albedo', 'alhaitham', 'aloy', 'amber', 'ayaka', 'ayato', 'baizhu', 'barbara', 'beidou', 'bennett', 'candace', 'charlotte', 'chevreuse', 'childe', 'chiori', 'chongyun', 'collei', 'cyno', 'dehya', 'diluc', 'diona', 'dori', 'eula', 'faruzan', 'fischl', 'freminet', 'furina', 'gaming', 'ganyu', 'gorou', 'heizou', 'hutao', 'itto', 'jean', 'kaeya', 'kaveh', 'kazuha', 'keqing', 'kirara', 'klee', 'kokomi', 'kukishinobu', 'layla', 'lisa', 'lynette', 'lyney', 'mika', 'mona', 'nahida', 'navia', 'neuvillette', 'nilou', 'ningguang', 'noelle', 'qiqi', 'raiden', 'razor', 'rosaria', 'sara', 'sayu', 'shenhe', 'sucrose', 'thoma', 'tighnari', 'traveler(anemo)', 'traveler(dendro)', 'traveler(electro)', 'traveler(geo)', 'traveler(hydro)', 'venti', 'wanderer', 'wriothesley', 'xiangling', 'xianyun', 'xiao', 'xingqiu', 'xinyan', 'yaemiko', 'yanfei', 'yaoyao', 'yelan', 'yoimiya', 'yunjin', 'zhongli']\n"
     ]
    }
   ],
   "source": [
    "characters_gg = [element.get_text() for element in imgtag_gg]\n",
    "#characters\n",
    "\n",
    "def process_names(names_list):\n",
    "    return [name.replace(\" \", \"\").lower() for name in names_list]\n",
    "\n",
    "characters_gg = process_names(characters_gg)\n",
    "characters_gg = sorted(characters_gg)\n",
    "print(characters_gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f39ea",
   "metadata": {},
   "source": [
    "### Tidying Up Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6096007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['albedo',\n",
       " 'alhaitham',\n",
       " 'aloy',\n",
       " 'amber',\n",
       " 'ayaka',\n",
       " 'ayato',\n",
       " 'baizhu',\n",
       " 'barbara',\n",
       " 'beidou',\n",
       " 'bennett',\n",
       " 'candace',\n",
       " 'charlotte',\n",
       " 'chevreuse',\n",
       " 'childe',\n",
       " 'chiori',\n",
       " 'chongyun',\n",
       " 'collei',\n",
       " 'cyno',\n",
       " 'dehya',\n",
       " 'diluc',\n",
       " 'diona',\n",
       " 'dori',\n",
       " 'eula',\n",
       " 'faruzan',\n",
       " 'fischl',\n",
       " 'freminet',\n",
       " 'furina',\n",
       " 'gaming',\n",
       " 'ganyu',\n",
       " 'gorou',\n",
       " 'heizou',\n",
       " 'hutao',\n",
       " 'itto',\n",
       " 'jean',\n",
       " 'kaeya',\n",
       " 'kaveh',\n",
       " 'kazuha',\n",
       " 'keqing',\n",
       " 'kirara',\n",
       " 'klee',\n",
       " 'kokomi',\n",
       " 'kukishinobu',\n",
       " 'layla',\n",
       " 'lisa',\n",
       " 'lynette',\n",
       " 'lyney',\n",
       " 'mika',\n",
       " 'mona',\n",
       " 'nahida',\n",
       " 'navia',\n",
       " 'neuvillette',\n",
       " 'nilou',\n",
       " 'ningguang',\n",
       " 'noelle',\n",
       " 'qiqi',\n",
       " 'raiden',\n",
       " 'razor',\n",
       " 'rosaria',\n",
       " 'sara',\n",
       " 'sayu',\n",
       " 'shenhe',\n",
       " 'sucrose',\n",
       " 'thoma',\n",
       " 'tighnari',\n",
       " 'traveler(anemo)',\n",
       " 'traveler(dendro)',\n",
       " 'traveler(electro)',\n",
       " 'traveler(geo)',\n",
       " 'traveler(hydro)',\n",
       " 'venti',\n",
       " 'wanderer',\n",
       " 'wriothesley',\n",
       " 'xiangling',\n",
       " 'xianyun',\n",
       " 'xiao',\n",
       " 'xingqiu',\n",
       " 'xinyan',\n",
       " 'yaemiko',\n",
       " 'yanfei',\n",
       " 'yaoyao',\n",
       " 'yelan',\n",
       " 'yoimiya',\n",
       " 'yunjin',\n",
       " 'zhongli']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#string_to_remove = ['itto', 'kazuha', 'sara', 'kokomi', 'ayato', 'ayaka', 'childe']\n",
    "#filtered_list1 = [item for item in characters_gg if item != string_to_remove]\n",
    "\n",
    "#'Arataki_Itto', 'Kaedehara_Kazuha', \"Sangonomiya_Kokomi\", \"Kujo_Sara\", 'Kamisato_Ayato', 'Kamisato_Ayaka', \n",
    "#'Lumine', 'Traveler_(Unaligned)', Tartaglia\n",
    "\n",
    "#filtered_list1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8c0ce",
   "metadata": {},
   "source": [
    "### Scraping Constellation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8c1266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Scraping Constellation Data from Genshin.gg - Complete\n",
    "\n",
    "characters_gg_test = characters_gg[2:5]\n",
    "cons_full = []\n",
    "\n",
    "for names in characters_gg_test:\n",
    "    \n",
    "    url_chara_gg = \"https://genshin.gg/characters/\"+names+\"/\"\n",
    "    page_chara_gg = requests.get(url_chara_gg)\n",
    "    soup_chara_gg = BeautifulSoup(page_chara_gg.content, \"html.parser\")\n",
    " \n",
    "\n",
    "    #Obtaining the constellation html info\n",
    "    \n",
    "    if soup_chara_gg.find(\"div\", id='constellations') != None:\n",
    "        cons1 = soup_chara_gg.find(\"div\", id=\"constellations\").find_all(\"div\", class_=\"character-skill-description\")\n",
    "        cons2 = [con.get_text() for con in cons1]\n",
    "        #print(cons2)\n",
    "        cons_full.append(cons2)\n",
    "    \n",
    "    else:\n",
    "        #print(\"No Constellations\")\n",
    "        notext = \"This character has no constellations.\"\n",
    "        cons_full.append(notext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa2760ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This character has no constellations.',\n",
       " [\"When Kamisato Ayaka's Normal or Charged Attacks deal Cryo DMG to opponents, it has a 50% chance of decreasing the CD of Kamisato Art: Hyouka by 0.3s. This effect can occur once every 0.1s.\",\n",
       "  \"When casting Kamisato Art: Soumetsu, unleashes 2 smaller additional Frostflake Seki no To, each dealing 20% of the original storm's DMG.\",\n",
       "  'Increases the Level of Kamisato Art: Soumetsu by 3. Maximum upgrade level is 15.',\n",
       "  \"Opponents damaged by Kamisato Art: Soumetsu's Frostflake Seki no To will have their DEF decreased by 30% for 6s.\",\n",
       "  'Increase the Level of Kamisato Art: Hyouka by 3. Maximum upgrade level is 15.',\n",
       "  \"Kamisato Ayaka gains Usurahi Butou every 10s, increasing her Charged Attack DMG by 298%. This buff will be cleared 0.5s after Ayaka's Charged ATK hits an opponent, after which the timer for this ability will restart.\"],\n",
       " ['Shunsuiken DMG is increased by 40% against opponents with 50% HP or less.',\n",
       "  \"Namisen's maximum stack count is increased to 5. When Kamisato Ayato has at least 3 Namisen stacks, his Max HP is increased by 50%.\",\n",
       "  'Increases the Level of Kamisato Art: Kyouka by 3. Maximum upgrade level is 15.',\n",
       "  'After using Kamisato Art: Suiyuu, all nearby party members will have 15% increased Normal Attack SPD for 15s.',\n",
       "  'Increase the Level of Kamisato Art: Suiyuu by 3. Maximum upgrade level is 15.',\n",
       "  \"After using Kamisato Art: Kyouka, Ayato's next Shunsuiken attack will create 2 extra Shunsuiken strikes when they hit opponents, each one dealing 450% of Ayato's ATK as DMG. Both these Shunsuiken attacks will not be affected by Namisen.\"]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4448c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No release date found.\n",
      "No release date found.\n",
      "No release date found.\n",
      "No release date found.\n",
      "No release date found.\n",
      "No release date found.\n",
      "No release date found.\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the release date of the Character\n",
    "\n",
    "characters_wk_test = characters_wk[6]\n",
    "dates_full = []\n",
    "\n",
    "for names in characters_wk_test:\n",
    "\n",
    "    url_chara_wk = \"https://genshin-impact.fandom.com/wiki/\"+names\n",
    "    page_chara_wk = requests.get(url_chara_wk)\n",
    "    soup_chara_wk = BeautifulSoup(page_chara_wk.content, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    release_date_div = soup_chara_wk.find('div', {'data-source': 'releaseDate'})\n",
    "\n",
    "    if release_date_div:\n",
    "        release_date_text = release_date_div.find('div', class_='pi-data-value')\n",
    "        release_date_text = release_date_text.get_text(strip=True).split(' ')\n",
    "        output = release_date_text[0:3]\n",
    "    \n",
    "        if len(output) >= 3:                  # Check if there are at least 3 elements in the list\n",
    "            output[2] = output[2][:-1]\n",
    "\n",
    "        output = ' '.join(output)\n",
    "        output = datetime.strptime(output, \"%B %d, %Y\").date()\n",
    "        dates_full.append(output)\n",
    "        \n",
    "    else:\n",
    "        print(\"No release date found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80c01856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd8759",
   "metadata": {},
   "source": [
    "### Obtaining list of all classes used in Character's html page\n",
    "\n",
    "#class list set \n",
    "class_list = set() \n",
    "\n",
    "#Page content from Website URL \n",
    "page = requests.get(url_chara) \n",
    "\n",
    "#parse html content \n",
    "soup = BeautifulSoup(page.content , 'html.parser') \n",
    "\n",
    "#get all tags \n",
    "tags = {tag.name for tag in soup.find_all()} \n",
    "\n",
    "\n",
    "for tag in tags:                       # iterate all tags \n",
    "\n",
    "    for i in soup.find_all( tag ):     # find all element of tag \n",
    " \n",
    "        if i.has_attr( \"class\" ):      # if tag has attribute of class\n",
    "\n",
    "            if len( i['class'] ) != 0: \n",
    "                class_list.add(\" \".join( i['class'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da5d22",
   "metadata": {},
   "source": [
    "#print(class_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of the character's constellations.\n",
    "\n",
    "cons3 = []\n",
    "\n",
    "for data in cons2:\n",
    "    \n",
    "    #soup = BeautifulSoup(data, 'html.parser')\n",
    "    #print(type(data))\n",
    "    #li_tag = data.find('li')\n",
    "    \n",
    "    if data:\n",
    "        data = data.text.strip()\n",
    "        #print(data)\n",
    "        cons3.append(data)\n",
    "    else:\n",
    "        print(\"No tag found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cons3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16ae2235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release Date: 2021-12-14\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the release date of the Character\n",
    "\n",
    "#print(release_date_div)\n",
    "\n",
    "\n",
    "url_chara_wk = \"https://genshin-impact.fandom.com/wiki/\"+characters_wk[4]\n",
    "page_chara_wk = requests.get(url_chara_wk)\n",
    "soup_chara_wk = BeautifulSoup(page_chara_wk.content, \"html.parser\")\n",
    "\n",
    "\n",
    "release_date_div = soup_chara_wk.find('div', {'data-source': 'releaseDate'})\n",
    "\n",
    "\n",
    "if release_date_div:\n",
    "    release_date_text = release_date_div.find('div', class_='pi-data-value')\n",
    "    release_date_text = release_date_text.get_text(strip=True).split(' ')\n",
    "    \n",
    "    #print(release_date_text)\n",
    "    #str(release_date_text)\n",
    "        \n",
    "    output = release_date_text[0:3]\n",
    "    \n",
    "    if len(output) >= 3:  # Check if there are at least 3 elements in the list\n",
    "        output[2] = output[2][:-1]\n",
    "\n",
    "    output = ' '.join(output)\n",
    "    output = datetime.strptime(output, \"%B %d, %Y\").date()\n",
    "    \n",
    "    #output\n",
    "    \n",
    "    print(\"Release Date:\", output)\n",
    "    \n",
    "else:\n",
    "    print(\"No release date found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b625a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
